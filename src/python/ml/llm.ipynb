{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement self attention mechanism\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value\n",
    "        self.scale = torch.sqrt(torch.tensor(embed_dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, embed_dim)\n",
    "        mask: (batch_size, seq_len, seq_len) - Optional mask for attention\n",
    "        \"\"\"\n",
    "        Q = self.W_q(x)  # (batch_size, seq_len, embed_dim)\n",
    "        K = self.W_k(x)  # (batch_size, seq_len, embed_dim)\n",
    "        V = self.W_v(x)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # Compute attention scores (batch_size, seq_len, seq_len)\n",
    "        attn_scores = torch.bmm(Q, K.transpose(1, 2)) / self.scale\n",
    "\n",
    "        # Apply mask (for causal masking or padding tokens)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Compute weighted values\n",
    "        output = torch.bmm(attn_weights, V)  # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return output, attn_weights\n",
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "embed_dim = 8\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)  # Random input\n",
    "self_attn = SelfAttention(embed_dim)\n",
    "output, attn_weights = self_attn(x)\n",
    "\n",
    "print(\"Output Shape:\", output.shape)  # (batch_size, seq_len, embed_dim)\n",
    "print(\"Attention Weights Shape:\", attn_weights.shape)  # (batch_size, seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention Implementation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        \"\"\"\n",
    "        tgt: (seq_len, batch_size, embed_dim) - Target sequence input\n",
    "        memory: (seq_len, batch_size, embed_dim) - Encoder output\n",
    "        tgt_mask: (seq_len, seq_len) - Causal mask for autoregressive decoding\n",
    "        memory_mask: (seq_len, seq_len) - Mask for encoder-decoder attention\n",
    "        \"\"\"\n",
    "        # Self-Attention (Masked)\n",
    "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
    "\n",
    "        # Cross-Attention (Attend to Encoder Output)\n",
    "        tgt2 = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
    "\n",
    "        # Feedforward Network\n",
    "        tgt2 = self.feedforward(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
    "\n",
    "        return tgt\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, vocab_size, max_seq_len, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_seq_len, embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_seq_len, embed_dim):\n",
    "        \"\"\" Generate sinusoidal positional encoding \"\"\"\n",
    "        position = torch.arange(max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
    "        pos_enc = torch.zeros(max_seq_len, embed_dim)\n",
    "        pos_enc[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_enc[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_enc.unsqueeze(1)  # Shape: (seq_len, 1, embed_dim)\n",
    "\n",
    "    def generate_mask(self, seq_len):\n",
    "        \"\"\" Generate causal mask for self-attention \"\"\"\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "        return mask.masked_fill(mask == 1, float('-inf'))\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: (batch_size, seq_len) - Target token indices\n",
    "        memory: (seq_len, batch_size, embed_dim) - Encoder outputs\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = tgt.shape\n",
    "        tgt_embed = self.embedding(tgt) + self.positional_encoding[:seq_len, :]\n",
    "\n",
    "        # Reshape to (seq_len, batch_size, embed_dim) for MHA compatibility\n",
    "        tgt_embed = tgt_embed.permute(1, 0, 2)  # Transpose to match (seq_len, batch_size, embed_dim)\n",
    "        tgt_mask = self.generate_mask(seq_len).to(tgt.device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            tgt_embed = layer(tgt_embed, memory, tgt_mask)\n",
    "\n",
    "        logits = self.fc_out(tgt_embed)  # Convert to vocabulary probabilities\n",
    "        return logits.permute(1, 0, 2)  # Reshape back to (batch_size, seq_len, vocab_size)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
